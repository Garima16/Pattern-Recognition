{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Random Samples\n",
    "\n",
    "We will now generate samples of the Gaussian Distribution with $\\mathcal{N}(\\mu,\\Sigma).$ For this we first need to generate the $\\mu$ and $\\Sigma$ matrices.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.datasets as skdataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMeanAndCovMatrices(dim):\n",
    "    cov_mat=skdataset.make_spd_matrix(dim)\n",
    "    mean_vec=np.random.randn(1,dim)[0]\n",
    "    return mean_vec, cov_mat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generateRandomSamples(dim,num_of_samples):\n",
    "    mean_vec, cov_mat = getMeanAndCovMatrices(dim)\n",
    "    return np.random.multivariate_normal(mean_vec, cov_mat,(num_of_samples)).reshape(dim,num_of_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating random samples of 5 dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.71916817, -0.47791457, -0.35816703, -0.91663054, -0.55005985,\n",
       "        -0.37910887, -0.83788672],\n",
       "       [ 0.66560852, -0.14836529, -1.23325409, -1.17185418, -1.36653755,\n",
       "         0.11842697, -0.31956973],\n",
       "       [ 1.19289474, -0.03016004, -0.30509029, -0.26879544,  0.18675589,\n",
       "        -2.26668049, -0.48347193],\n",
       "       [ 1.12593443,  3.67953803,  2.58270316, -4.04189417,  0.57756252,\n",
       "        -0.1774156 ,  0.92706617],\n",
       "       [ 1.30659482, -0.03251858,  0.01434854,  0.78117878,  1.03329985,\n",
       "         0.64618492, -1.89288452]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generateRandomSamples(5,7) #generates 7 samples of whose dimension = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procedure to claculate Discriminant Function\n",
    "\n",
    "For a given Normal Distribution $\\mathcal{N}(\\mu,\\Sigma).$ and Prior probability $P(\\omega_i)$ the discriminant function is given by the equation\n",
    "\n",
    "$g_i(x)$ = $-\\frac{1}{2}(x-\\mu)^t\\Sigma^{-1}(x-\\mu)-\\frac{d}{2}ln(2\\pi)-\\frac{1}{2}ln|\\Sigma_i|+ln(P(\\omega_i)))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminant_function(dimension,x_vec,mean_vec,cov_mat,prior):\n",
    "    if dimension > 1:\n",
    "        cov_det = np.linalg.det(cov_mat)\n",
    "        return ((-1 / 2) * mahalanobis(x_vec,mean_vec,cov_mat) \\\n",
    "                - (dimension / 2) * np.log(2 * np.pi) \\\n",
    "                - (1 / 2) * np.log(cov_det) + np.log(prior))\n",
    "    else:\n",
    "        return ((-1 / 2) * euclidean(x_vec,mean_vec) / cov_mat \\\n",
    "                - (1 / 2) * np.log(2 * np.pi * cov_mat) \\\n",
    "                + np.log(prior))                                                                               \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mahalanobis Distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mahalanobis(x_vec,mean_vec,cov_mat):\n",
    "    difference = x_vec - mean_vec\n",
    "    cov_inv = np.linalg.inv(cov_mat)\n",
    "    return np.dot(np.dot(difference.T,cov_inv),difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Euclidean Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean(p1,p2):\n",
    "    return mahalanobis(p1,p2,np.identity(p1.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading dataset from csv file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.genfromtxt(\"./synthetic_data.csv\",delimiter=',', skip_header=1)\n",
    "class1_data=data[:10]\n",
    "class2_data=data[10:20]\n",
    "class3_data=data[20:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univarite and Multivariate Excersises: (2a , 2b, 2c, 2d)\n",
    "Given prior probabilities are $P(\\omega_1)=P(\\omega_2)=0.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def dichotomiser(feature_dim, features, labels, prior1, prior2):\n",
    "    class1_mean = np.mean(features[:10],axis=0).reshape(feature_dim,1)\n",
    "    class2_mean = np.mean(features[10:20],axis=0).reshape(feature_dim,1)\n",
    "    \n",
    "    if(feature_dim > 1):\n",
    "        class1_covariance = np.cov(features[:10].T)\n",
    "        class2_covariance = np.cov(features[10:20].T)\n",
    "    else:\n",
    "        class1_covariance = np.var(features[:10],axis=0)\n",
    "        class2_covariance = np.var(features[10:20],axis=0)\n",
    "    \n",
    "    class1_prior = prior1\n",
    "    class2_prior = prior2\n",
    "    \n",
    "    prediction = []\n",
    "    for feature in features:\n",
    "        feature = feature.reshape(feature_dim,1)\n",
    "        \n",
    "        g1 = discriminant_function(feature_dim, feature, class1_mean, class1_covariance,class1_prior)\n",
    "        g2 = discriminant_function(feature_dim, feature, class2_mean, class2_covariance,class2_prior)        \n",
    "        \n",
    "        if(g1 - g2 > 0):\n",
    "            prediction.append(0)\n",
    "        elif(g1 - g2 < 0):\n",
    "            prediction.append(1)\n",
    "    print_predictions(labels, prediction)\n",
    "    print(\"The Empirical Training error is : \", empirical_error(labels, prediction), \"%\")  \n",
    "        \n",
    "def print_predictions(actual,prediction):\n",
    "    print(\"Actual\\t\\t:\\t\", actual)\n",
    "    print(\"Predicted\\t:\\t\", prediction)\n",
    "    \n",
    "        \n",
    "def empirical_error(actual,prediction):\n",
    "    error_count=0\n",
    "    for i,val in enumerate(prediction):\n",
    "        if(actual[i] - val):\n",
    "            error_count += 1\n",
    "    return (error_count/actual.shape[0]) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual\t\t:\t [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Predicted\t:\t [1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1]\n",
      "The Empirical Training error is :  35.0 %\n"
     ]
    }
   ],
   "source": [
    "dichotomiser(1, data[:20,0], data[:20,3], 0.5, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual\t\t:\t [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Predicted\t:\t [0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0]\n",
      "The Empirical Training error is :  45.0 %\n"
     ]
    }
   ],
   "source": [
    "dichotomiser(2, data[:20,:2], data[:20,3], 0.5, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual\t\t:\t [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Predicted\t:\t [0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1]\n",
      "The Empirical Training error is :  15.0 %\n"
     ]
    }
   ],
   "source": [
    "dichotomiser(3, data[:20,:3], data[:20,3], 0.5, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2e. Claculating the test points Mahalanobis Distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 1],\n",
       "       [5, 3, 2],\n",
       "       [0, 0, 0],\n",
       "       [1, 0, 0]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_points = np.genfromtxt(\"./test-points.csv\",delimiter=\",\",skip_header=1,dtype=int)\n",
    "test_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted classes based on Mahalanobis distance are :  [1, 2, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "feature_dim=3\n",
    "\n",
    "class1_mean = np.mean(class1_data[:,:3],axis=0).reshape(feature_dim,1)\n",
    "class2_mean = np.mean(class2_data[:,:3],axis=0).reshape(feature_dim,1)\n",
    "class3_mean = np.mean(class3_data[:,:3],axis=0).reshape(feature_dim,1)\n",
    "\n",
    "class1_covariance = np.cov(class1_data[:,:3].T)\n",
    "class2_covariance = np.cov(class2_data[:,:3].T)\n",
    "class3_covariance = np.cov(class3_data[:,:3].T)\n",
    "\n",
    "prediction = []\n",
    "for test_point in test_points:\n",
    "    test_point=test_point.reshape(feature_dim,1)\n",
    "    distances = []\n",
    "    distances.append(mahalanobis(test_point,class1_mean,class1_covariance))\n",
    "    distances.append(mahalanobis(test_point,class2_mean,class2_covariance))    \n",
    "    distances.append(mahalanobis(test_point,class3_mean,class3_covariance))\n",
    "    prediction.append(distances.index(min(distances)))\n",
    "print(\"Predicted classes based on Mahalanobis distance are : \",prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2f. With change in Prior Probabilities\n",
    "\n",
    "Now considering the prior probabilities of each of the class as $P(\\omega_1) = 0.8, P(\\omega_2)=0.1, P(\\omega_3)=0.1$, predicting the class based on Mahalanobis distance has no effect as it is independent of prior probabilities. But considering the maximum value of LDF we get all the test points in class '0'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted classes are :  [0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "prediction = []\n",
    "for test_point in test_points:\n",
    "    test_point=test_point.reshape(feature_dim,1)\n",
    "    values = []\n",
    "    values.append(discriminant_function(feature_dim,test_point,class1_mean,class1_covariance,0.8))\n",
    "    values.append(discriminant_function(feature_dim,test_point,class2_mean,class2_covariance,0.1))    \n",
    "    values.append(discriminant_function(feature_dim,test_point,class3_mean,class3_covariance,0.1))\n",
    "    prediction.append(values.index(max(values)))\n",
    "print(\"Predicted classes are : \",prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_dataset = np.genfromtxt(\"iris.csv\",delimiter=\",\")\n",
    "labels=[\"Iris-setosa\",\"Iris-versicolor\",\"Iris-virginica\"]\n",
    "\n",
    "iris_class1 = np.insert(iris_dataset[:50,:4],4,0,axis=1)\n",
    "iris_class2 = np.insert(iris_dataset[50:100,:4],4,1,axis=1)\n",
    "iris_class3 = np.insert(iris_dataset[100:150,:4],4,2,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dim=4\n",
    "iris_samples1 = iris_class1[:,:4]\n",
    "iris_samples2 = iris_class2[:,:4]\n",
    "iris_samples3 = iris_class3[:,:4]\n",
    "\n",
    "iris_class1_mean = np.mean(iris_samples1,axis=0).reshape(feature_dim,1)\n",
    "iris_class2_mean = np.mean(iris_samples2,axis=0).reshape(feature_dim,1)\n",
    "iris_class3_mean = np.mean(iris_samples3,axis=0).reshape(feature_dim,1)\n",
    "\n",
    "#Case III Covariance matrices:\n",
    "iris_class1_covariance = np.cov(iris_samples1.T)\n",
    "iris_class2_covariance = np.cov(iris_samples2.T)\n",
    "iris_class3_covariance = np.cov(iris_samples3.T)\n",
    "\n",
    "#Case I Covariance matrix:\n",
    "var1 = np.mean(iris_class1_covariance.diagonal())\n",
    "var2 = np.mean(iris_class2_covariance.diagonal())\n",
    "var3 = np.mean(iris_class3_covariance.diagonal())\n",
    "average_var = (var1 + var2 + var3)/3\n",
    "cov_mat1 = average_var*np.identity(feature_dim)\n",
    "\n",
    "#Case II Covariance matrix:\n",
    "cov_mat2 = (1/3 )* (iris_class1_covariance + iris_class2_covariance + iris_class3_covariance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iris Data Case 1\n",
    "The LDF for Case 1 is given by the equation: \n",
    "\n",
    "$g_i(x) = W_i^Tx+W_{i0} ------- (1)$\n",
    "\n",
    "where $W_i= \\frac{\\mu_i}{\\sigma^2} \\implies W_i^T= \\frac{\\mu_i^T}{\\sigma^2}$\n",
    "\n",
    "and $W_{i0}= \\frac{-\\mu_i^T\\mu_i}{2\\sigma^2}+ ln(P(\\omega_i))$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating  $W_i^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[32.93023131 22.48412517  9.63041523  1.6050692 ]]\n",
      "[[39.04791311 18.22148237 28.02292956  8.72263019]]\n",
      "[[43.33686853 19.56342547 36.52190256 13.32733692]]\n"
     ]
    }
   ],
   "source": [
    "w1_t = iris_class1_mean.T / average_var\n",
    "w2_t = iris_class2_mean.T / average_var\n",
    "w3_t = iris_class3_mean.T / average_var\n",
    "print(w1_t,w2_t,w3_t,sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating $W_{i0}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-129.19363357]]\n",
      "[[-207.70151527]]\n",
      "[[-287.82646472]]\n"
     ]
    }
   ],
   "source": [
    "w10 = (- 0.5) * (np.dot(iris_class1_mean.T,iris_class1_mean) / average_var) + np.log(1/3)\n",
    "w20 = (- 0.5) * (np.dot(iris_class2_mean.T,iris_class2_mean) / average_var) + np.log(1/3)\n",
    "w30 = (- 0.5) * (np.dot(iris_class3_mean.T,iris_class3_mean) / average_var) + np.log(1/3)\n",
    "print(w10,w20,w30,sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By substituting the values of $W_i^T$ and $W_{i0}$ in equation (1) we get the Case 1 LDF for Iris dataset\n",
    "\n",
    "$g_1(x) = 32.93* x_1 + 22.48 * x_2 + 9.63 * x_3 + 1.60 *x_4 - 129.19$\n",
    "\n",
    "$g_2(x) = 39.05* x_1 + 18.22 * x_2 + 28.02 * x_3 + 8.72 *x_4 - 207.70$\n",
    "\n",
    "$g_3(x) = 43.34* x_1 + 19.56 * x_2 + 36.52 * x_3 + 13.33 *x_4 - 287.83$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iris Data Case 2\n",
    "The LDF for Case 2 is given by the equation: \n",
    "\n",
    "$g_i(x) = W_i^Tx+W_{i0} ------------ (2)$ \n",
    "\n",
    "where $W_i= \\Sigma^{-1}\\mu_i \\implies  W_i^T= \\mu_i^T(\\Sigma^{-1})^T$\n",
    "\n",
    "and $W_{i0}= \\frac{-1}{2}\\mu_i^T\\Sigma^{-1}\\mu_i+ ln(P(\\omega_i))$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating $W_i^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 23.46638411  23.56828007 -16.20296668 -18.02533894]]\n",
      "[[15.70349917  6.95425598  5.28429325  6.29830031]]\n",
      "[[12.49061995  3.44398145 12.82207622 21.06272174]]\n"
     ]
    }
   ],
   "source": [
    "w1_t = np.dot(iris_class1_mean.T,np.linalg.inv(cov_mat2).T)\n",
    "w2_t = np.dot(iris_class2_mean.T,np.linalg.inv(cov_mat2).T)\n",
    "w3_t = np.dot(iris_class3_mean.T,np.linalg.inv(cov_mat2).T)\n",
    "print(w1_t,w2_t,w3_t,sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculatin $W_{i0}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-86.05349939]]\n",
      "[[-72.76956007]]\n",
      "[[-104.2945355]]\n"
     ]
    }
   ],
   "source": [
    "w10 = (- 0.5) * np.dot(np.dot(iris_class1_mean.T,np.linalg.inv(cov_mat2)),iris_class1_mean) + np.log(1/3)\n",
    "w20 = (- 0.5) * np.dot(np.dot(iris_class2_mean.T,np.linalg.inv(cov_mat2)),iris_class2_mean) + np.log(1/3)\n",
    "w30 = (- 0.5) * np.dot(np.dot(iris_class3_mean.T,np.linalg.inv(cov_mat2)),iris_class3_mean) + np.log(1/3)\n",
    "print(w10,w20,w30,sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By substituting the values of $W_i^T$ and $W_{i0}$ in equation (2) we get the Case 2 LDF for Iris dataset\n",
    "\n",
    "\n",
    "$g_1(x) = 23.46* x_1 + 23.57 * x_2 - 16.20 * x_3 - 18.025 *x_4 - 86.05$\n",
    "\n",
    "$g_2(x) = 15.70* x_1 + 6.95 * x_2 + 5.28 * x_3 + 6.29 *x_4 - 72.77$\n",
    "\n",
    "$g_3(x) = 12.49* x_1 + 3.44 * x_2 + 12.82* x_3 + 21.06*x_4 - 104.294$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iris Case 3\n",
    "The QDF for Case 3 is given by the equation: \n",
    "\n",
    "$\n",
    "g_i(x) = x^tW_ix + w_i^tx + w_{i0} ------ (3)\n",
    "$\n",
    "\n",
    "Where,\n",
    "\n",
    "$\n",
    "W_i = -\\frac{1}{2}\\Sigma_i\n",
    "$\n",
    "\n",
    "$\n",
    "w_i = \\Sigma_i^{-1}\\mu_i\n",
    "$\n",
    "\n",
    "$\n",
    "w_{i0} = -\\frac{1}{2}\\mu^t_i\\Sigma_i^{-1}\\mu_i  -\\frac{1}{2}\\ln \\lvert \\Sigma_i \\rvert + \\ln(P(\\omega_i))\n",
    "$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating ${W_i}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.06212449 -0.05014898 -0.00806939 -0.00527347]\n",
      " [-0.05014898 -0.0725898  -0.00584082 -0.00571837]\n",
      " [-0.00806939 -0.00584082 -0.01505306 -0.00284898]\n",
      " [-0.00527347 -0.00571837 -0.00284898 -0.00574694]]\n",
      "\n",
      "[[-0.13321633 -0.04259184 -0.09144898 -0.0278898 ]\n",
      " [-0.04259184 -0.04923469 -0.04132653 -0.02060204]\n",
      " [-0.09144898 -0.04132653 -0.11040816 -0.03655102]\n",
      " [-0.0278898  -0.02060204 -0.03655102 -0.01955306]]\n",
      "\n",
      "[[-0.20217143 -0.04688163 -0.1516449  -0.02454694]\n",
      " [-0.04688163 -0.05200204 -0.0356898  -0.02381429]\n",
      " [-0.1516449  -0.0356898  -0.15229388 -0.02441224]\n",
      " [-0.02454694 -0.02381429 -0.02441224 -0.03771633]]\n"
     ]
    }
   ],
   "source": [
    "W1 = (- 0.5) * iris_class1_covariance\n",
    "W2 = (- 0.5) * iris_class2_covariance\n",
    "W3 = (- 0.5) * iris_class3_covariance\n",
    "print(W1,W2,W3,sep=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating $w_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 44.6351704   -7.71338075  33.07861577 -28.45246274]]\n",
      "[[ 18.0128645   15.96070005   3.26878502 -14.71255747]]\n",
      "[[ 7.37247478 13.2452613   6.23406948  9.66197608]]\n"
     ]
    }
   ],
   "source": [
    "w1_t = np.dot(iris_class1_mean.T,np.linalg.inv(iris_class1_covariance).T)\n",
    "w2_t = np.dot(iris_class2_mean.T,np.linalg.inv(iris_class2_covariance).T)\n",
    "w3_t = np.dot(iris_class3_mean.T,np.linalg.inv(iris_class3_covariance).T)\n",
    "print(w1_t,w2_t,w3_t,sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating $W_{i0}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-113.86317185]]\n",
      "[[-68.43728769]]\n",
      "[[-67.7090772]]\n"
     ]
    }
   ],
   "source": [
    "w10 = (- 0.5 * np.dot(np.dot(iris_class1_mean.T,np.linalg.inv(iris_class1_covariance)),iris_class1_mean)) - (0.5 * np.log(np.linalg.det(iris_class1_covariance))) + np.log(1/3)\n",
    "w20 = (- 0.5 * np.dot(np.dot(iris_class2_mean.T,np.linalg.inv(iris_class2_covariance)),iris_class2_mean)) - (0.5 * np.log(np.linalg.det(iris_class2_covariance))) + np.log(1/3)\n",
    "w30 = (- 0.5 * np.dot(np.dot(iris_class3_mean.T,np.linalg.inv(iris_class3_covariance)),iris_class3_mean)) - (0.5 * np.log(np.linalg.det(iris_class3_covariance))) + np.log(1/3)\n",
    "print(w10,w20,w30,sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By substituting these values of $W_i$ and $w_i$ and $W_{i0}$ in equation 3 we get the Quadratic Discriminant Function for Iris dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
